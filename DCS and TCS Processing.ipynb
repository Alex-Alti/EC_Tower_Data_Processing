{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9996bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d61bb",
   "metadata": {},
   "source": [
    "# DCS Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa67c301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrale\\AppData\\Local\\Temp\\ipykernel_41024\\139666611.py:22: DtypeWarning: Columns (167) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "C:\\Users\\mrale\\AppData\\Local\\Temp\\ipykernel_41024\\139666611.py:22: DtypeWarning: Columns (30,32,49,50,51,53,54,55,56,57) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported merged and filtered data with lat and long to C:/Users/mrale/OneDrive/Documents/DCSTower\\merged_filtered_with_lat_long_date.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file path and pattern to match the CSV files\n",
    "file_path = \"C:/Users/mrale/OneDrive/Documents/DCSTower\"\n",
    "file_pattern = os.path.join(file_path, \"*.csv\")\n",
    "\n",
    "# Latitude and longitude values to add\n",
    "latitude = 25.7626\n",
    "longitude = -80.9078\n",
    "\n",
    "# Get a list of all CSV files matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "# Check if any CSV files were found\n",
    "if not csv_files:\n",
    "    raise ValueError(\"No CSV files found in the specified directory.\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        if \"FC\" in df.columns and \"TA\" in df.columns:\n",
    "            # Add latitude and longitude columns\n",
    "            df[\"Lat\"] = latitude\n",
    "            df[\"Long\"] = longitude\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping file {file} as it does not contain the required columns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Check if any DataFrames were successfully read\n",
    "if not dfs:\n",
    "    raise ValueError(\"No valid DataFrames to concatenate. Check the files for the required columns.\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop empty temp and flux columns\n",
    "temperature_columns = ['TA']\n",
    "df[temperature_columns] = df[temperature_columns].replace(-9999, np.nan)\n",
    "df['FC'] = df['FC'].replace(-9999, np.nan)\n",
    "\n",
    "# Convert temperature from Kelvin to Celsius\n",
    "merged_df[\"TA\"] = merged_df[\"TA\"] - 273.15\n",
    "\n",
    "# Select only the columns \"FC\", \"TA\", \"Lat\", and \"Long\"\n",
    "filtered_df = merged_df[[\"FC\", \"TA\", \"Lat\", \"Long\", \"date\"]]\n",
    "\n",
    "# Export the final DataFrame to a single CSV file\n",
    "output_file = os.path.join(file_path, \"merged_filtered_with_lat_long_date.csv\")\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Exported merged and filtered data with lat and long to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f2ef8",
   "metadata": {},
   "source": [
    "# TCS Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "005718bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported merged and filtered data with lat and long to C:/Users/mrale/OneDrive/Documents/TCSTower\\merged_filtered_with_lat_long_date.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file path and pattern to match the CSV files\n",
    "file_path = \"C:/Users/mrale/OneDrive/Documents/TCSTower\"\n",
    "file_pattern = os.path.join(file_path, \"*.csv\")\n",
    "\n",
    "# Latitude and longitude values to add\n",
    "latitude = 25.8221\n",
    "longitude = -81.1017\n",
    "\n",
    "# Get a list of all CSV files matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "# Check if any CSV files were found\n",
    "if not csv_files:\n",
    "    raise ValueError(\"No CSV files found in the specified directory.\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        if \"FC\" in df.columns and \"TA\" in df.columns:\n",
    "            # Add latitude and longitude columns\n",
    "            df[\"Lat\"] = latitude\n",
    "            df[\"Long\"] = longitude\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping file {file} as it does not contain the required columns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Check if any DataFrames were successfully read\n",
    "if not dfs:\n",
    "    raise ValueError(\"No valid DataFrames to concatenate. Check the files for the required columns.\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop empty temp and flux columns\n",
    "temperature_columns = ['TA']\n",
    "df[temperature_columns] = df[temperature_columns].replace(-9999, np.nan)\n",
    "df['FC'] = df['FC'].replace(-9999, np.nan)\n",
    "\n",
    "# Convert temperature from Kelvin to Celsius\n",
    "merged_df[\"TA\"] = merged_df[\"TA\"] - 273.15\n",
    "\n",
    "# Select only the columns \"FC\", \"TA\", \"Lat\", \"Long\", and \"date\"\n",
    "filtered_df = merged_df[[\"FC\", \"TA\", \"Lat\", \"Long\", \"date\"]]\n",
    "\n",
    "# Export the final DataFrame to a single CSV file\n",
    "output_file = os.path.join(file_path, \"merged_filtered_with_lat_long_date.csv\")\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Exported merged and filtered data with lat and long to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9334d889",
   "metadata": {},
   "source": [
    "# PIU Tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c8ca82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported merged and filtered data with lat and long to C:/Users/mrale/OneDrive/Documents/PIUTower\\merged_filtered_with_lat_long_date.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file path and pattern to match the CSV files\n",
    "file_path = \"C:/Users/mrale/OneDrive/Documents/PIUTower\"\n",
    "file_pattern = os.path.join(file_path, \"*.csv\")\n",
    "\n",
    "# Latitude and longitude values to add\n",
    "latitude = 26.0004\n",
    "longitude = -80.9261\n",
    "\n",
    "# Get a list of all CSV files matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "# Check if any CSV files were found\n",
    "if not csv_files:\n",
    "    raise ValueError(\"No CSV files found in the specified directory.\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        if \"FC\" in df.columns and \"TA\" in df.columns:\n",
    "            # Add latitude and longitude columns\n",
    "            df[\"Lat\"] = latitude\n",
    "            df[\"Long\"] = longitude\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"Skipping file {file} as it does not contain the required columns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Check if any DataFrames were successfully read\n",
    "if not dfs:\n",
    "    raise ValueError(\"No valid DataFrames to concatenate. Check the files for the required columns.\")\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop empty temp and flux columns\n",
    "temperature_columns = ['TA']\n",
    "df[temperature_columns] = df[temperature_columns].replace(-9999, np.nan)\n",
    "df['FC'] = df['FC'].replace(-9999, np.nan)\n",
    "\n",
    "# Select only the columns \"FC\", \"TA\", \"Lat\", \"Long\", and \"date\"\n",
    "filtered_df = merged_df[[\"FC\", \"TA\", \"Lat\", \"Long\", \"date\"]]\n",
    "\n",
    "# Convert temperature from Kelvin to Celsius\n",
    "merged_df[\"TA\"] = merged_df[\"TA\"] - 273.15\n",
    "\n",
    "# Export the final DataFrame to a single CSV file\n",
    "output_file = os.path.join(file_path, \"merged_filtered_with_lat_long_date.csv\")\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Exported merged and filtered data with lat and long to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd5d73",
   "metadata": {},
   "source": [
    "# EvM Tower - come back to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56ee2314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TIMESTAMP_START  TIMESTAMP_END      FC      TA      Lat     Long  \\\n",
      "0     2.020010e+11   2.020010e+11 -9999.0 -9999.0  25.5519 -80.7826   \n",
      "1     2.020010e+11   2.020010e+11 -9999.0 -9999.0  25.5519 -80.7826   \n",
      "2     2.020010e+11   2.020010e+11 -9999.0 -9999.0  25.5519 -80.7826   \n",
      "3     2.020010e+11   2.020010e+11 -9999.0 -9999.0  25.5519 -80.7826   \n",
      "4     2.020010e+11   2.020010e+11 -9999.0 -9999.0  25.5519 -80.7826   \n",
      "\n",
      "              Components  Year  Month  \n",
      "0  Year: 2020, Month: 01  2020      1  \n",
      "1  Year: 2020, Month: 01  2020      1  \n",
      "2  Year: 2020, Month: 01  2020      1  \n",
      "3  Year: 2020, Month: 01  2020      1  \n",
      "4  Year: 2020, Month: 01  2020      1  \n",
      "Exported merged and filtered data with lat and long to C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/EvM_gap_removed.csv\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/EvM_extracted_year_month.csv\"\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to inspect the 'Components' column\n",
    "print(df.head())\n",
    "\n",
    "# Replace -9999 with NaN in temperature and flux columns\n",
    "temperature_columns = ['TA']\n",
    "df[temperature_columns] = df[temperature_columns].replace(-9999, np.nan)\n",
    "df['FC'] = df['FC'].replace(-9999, np.nan)\n",
    "\n",
    "# Remove rows where 'TA_F' or 'FC' are NaN\n",
    "df = df.dropna(subset=['TA', 'FC'])\n",
    "\n",
    "# Split 'Components' column into 'Year' and 'Month' columns\n",
    "df[['Year', 'Month']] = df['Components'].str.extract(r'Year: (\\d+), Month: (\\d+)')\n",
    "\n",
    "# Convert 'Year' and 'Month' columns to integers\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['Month'] = df['Month'].astype(int)\n",
    "\n",
    "# Export the final DataFrame to a single CSV file\n",
    "file_path = \"C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/\"\n",
    "output_file = os.path.join(file_path, \"EvM_gap_removed.csv\")\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Exported merged and filtered data with lat and long to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b129a65",
   "metadata": {},
   "source": [
    "# Elm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5170ea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TIMESTAMP_START  TIMESTAMP_END      FC   TA_F      Lat     Long  \\\n",
      "0     201601010000   201601010030 -9999.0  23.70  25.5519 -80.7826   \n",
      "1     201601010030   201601010100 -9999.0  23.64  25.5519 -80.7826   \n",
      "2     201601010100   201601010130 -9999.0  23.52  25.5519 -80.7826   \n",
      "3     201601010130   201601010200 -9999.0  23.44  25.5519 -80.7826   \n",
      "4     201601010200   201601010230 -9999.0  23.38  25.5519 -80.7826   \n",
      "\n",
      "              Components  Year  Month  \n",
      "0  Year: 2016, Month: 01  2016      1  \n",
      "1  Year: 2016, Month: 01  2016      1  \n",
      "2  Year: 2016, Month: 01  2016      1  \n",
      "3  Year: 2016, Month: 01  2016      1  \n",
      "4  Year: 2016, Month: 01  2016      1  \n",
      "Exported merged and filtered data with lat and long to C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/Elm_gap_removed.csv\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "file_path = \"C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/Elm_extracted_year_month.csv\"\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to inspect the 'Components' column\n",
    "print(df.head())\n",
    "\n",
    "# Replace -9999 with NaN in temperature and flux columns\n",
    "temperature_columns = ['TA_F']\n",
    "df[temperature_columns] = df[temperature_columns].replace(-9999, np.nan)\n",
    "df['FC'] = df['FC'].replace(-9999, np.nan)\n",
    "\n",
    "# Remove rows where 'TA_F' or 'FC' are NaN\n",
    "df = df.dropna(subset=['TA_F', 'FC'])\n",
    "\n",
    "# Split 'Components' column into 'Year' and 'Month' columns\n",
    "df[['Year', 'Month']] = df['Components'].str.extract(r'Year: (\\d+), Month: (\\d+)')\n",
    "\n",
    "# Convert 'Year' and 'Month' columns to integers\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "df['Month'] = df['Month'].astype(int)\n",
    "\n",
    "# Export the final DataFrame to a single CSV file\n",
    "file_path = \"C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/\"\n",
    "output_file = os.path.join(file_path, \"Elm_gap_removed.csv\")\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Exported merged and filtered data with lat and long to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9730b",
   "metadata": {},
   "source": [
    "# All Towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2831da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data\\Elm_extracted_year_month.csv and exported cleaned data to C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/Elm_gap_removed.csv\n",
      "Processed C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data\\Esm_extracted_year_month.csv and exported cleaned data to C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/Esm_gap_removed.csv\n",
      "Processed C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data\\EvM_extracted_year_month.csv and exported cleaned data to C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/EvM_gap_removed.csv\n",
      "Processed C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data\\TaS_extracted_year_month.csv and exported cleaned data to C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/TaS_gap_removed.csv\n",
      "Processed C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data\\xDS_extracted_year_month.csv and exported cleaned data to C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/xDS_gap_removed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Folder path containing the CSV files\n",
    "folder_path = \"C:/Users/mrale/OneDrive/Documents/Cleaned_Tower_Data/\"\n",
    "\n",
    "# Get a list of all relevant CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*_extracted_year_month.csv\"))\n",
    "\n",
    "# Process each relevant CSV file\n",
    "for file_path in csv_files:\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Identify the correct temperature column\n",
    "    temperature_columns = ['TA_F', 'TA']\n",
    "    temp_col = next((col for col in temperature_columns if col in df.columns), None)\n",
    "    \n",
    "    # If no relevant temperature column is found, skip this file\n",
    "    if temp_col is None:\n",
    "        print(f\"No relevant temperature column found in {file_path}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Replace -9999 with NaN in temperature and flux columns\n",
    "    df[temp_col] = df[temp_col].replace(-9999, np.nan)\n",
    "    df['FC'] = df['FC'].replace(-9999, np.nan)\n",
    "\n",
    "    # Remove rows where temperature or flux are NaN\n",
    "    df = df.dropna(subset=[temp_col, 'FC'])\n",
    "\n",
    "    # Split 'Components' column into 'Year' and 'Month' columns\n",
    "    if 'Components' in df.columns:\n",
    "        df[['Year', 'Month']] = df['Components'].str.extract(r'Year: (\\d+), Month: (\\d+)')\n",
    "\n",
    "        # Convert 'Year' and 'Month' columns to integers\n",
    "        df['Year'] = df['Year'].astype(int)\n",
    "        df['Month'] = df['Month'].astype(int)\n",
    "    else:\n",
    "        print(f\"No 'Components' column found in {file_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Generate the output file name based on the input file name\n",
    "    file_name = os.path.basename(file_path)\n",
    "    site_code = file_name.split('_')[0]\n",
    "    output_file = os.path.join(folder_path, f\"{site_code}_gap_removed.csv\")\n",
    "\n",
    "    # Export the final DataFrame to a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Processed {file_path} and exported cleaned data to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209319f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
